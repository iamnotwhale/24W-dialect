{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamnotwhale/24W-dialect/blob/main/dialect_to_standard_with_kobart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "6FLGj8b5TT10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas tokenizers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUFNrETLTBIC",
        "outputId": "cfb39056-37c9-4211-ed28-b675ac76cc33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJuafnRpTDl8",
        "outputId": "9b3e95f4-6709-491b-f45e-f83370d43d5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m286.7/290.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T06:36:55.329078Z",
          "iopub.status.busy": "2024-03-22T06:36:55.328198Z",
          "iopub.status.idle": "2024-03-22T06:36:55.335624Z",
          "shell.execute_reply": "2024-03-22T06:36:55.334438Z",
          "shell.execute_reply.started": "2024-03-22T06:36:55.329045Z"
        },
        "trusted": true,
        "id": "teC4ObTPS5WS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "import os\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ0kJoRbTbsI",
        "outputId": "b8c4dfdb-6031-4279-995f-cf46512d570d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/daiv_audio\""
      ],
      "metadata": {
        "id": "nObLelaOUKub"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IfhpxiRS5Wb"
      },
      "source": [
        "## 모델 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T07:10:05.602141Z",
          "iopub.status.busy": "2024-03-22T07:10:05.601481Z",
          "iopub.status.idle": "2024-03-22T07:10:06.603257Z",
          "shell.execute_reply": "2024-03-22T07:10:06.602092Z",
          "shell.execute_reply.started": "2024-03-22T07:10:05.602106Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAVuWAtNS5Wb",
        "outputId": "d0abe463-2bac-47e9-a574-9f8c6292929a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use Pretrained Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n",
        "\n",
        "# 모델 학습 후에 가중치 저장할 폴더\n",
        "model_path=dir+'/saved_model'\n",
        "# 모델 초기 가중치 로드할 곳\n",
        "model_name = \"gogamza/kobart-base-v2\"\n",
        "# 데이터셋 파일\n",
        "data_root=dir\n",
        "\n",
        "# 내가 학습한 모델 가중치의 유무에 따라 분기 처리\n",
        "if os.path.exists(f'{model_path}/pytorch_model.bin'):\n",
        "    print(\"Use Customized Model\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "else:\n",
        "    print(\"Use Pretrained Model\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "# Training Arguments\n",
        "\n",
        "args = {\n",
        "    'num_train_epochs': 3,\n",
        "    'per_device_train_batch_size': 32,\n",
        "    'per_device_eval_batch_size': 32,\n",
        "    'overwrite_output_dir': True,\n",
        "    'eval_steps': 10000,\n",
        "    'save_steps': 10000,\n",
        "    'warmup_steps': 5,\n",
        "    'evaluation_strategy': \"steps\",\n",
        "    'prediction_loss_only': True,\n",
        "    'save_total_limit': 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T07:10:08.941497Z",
          "iopub.status.busy": "2024-03-22T07:10:08.940812Z",
          "iopub.status.idle": "2024-03-22T07:10:08.954774Z",
          "shell.execute_reply": "2024-03-22T07:10:08.953690Z",
          "shell.execute_reply.started": "2024-03-22T07:10:08.941460Z"
        },
        "trusted": true,
        "id": "jTKU44uhS5Wb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextStyleTransferDataset(Dataset):\n",
        "    def __init__(self, df,tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row=self.df.iloc[index]\n",
        "        text1=row[0] #표준어\n",
        "        text2=row[1] #사투리\n",
        "        target_style_name = '표준어'\n",
        "\n",
        "        # Tokenizer를 허깅페이스의 레포지토리에서 가져왔기 때문에\n",
        "        # 테스트 코드를 참고해서 모델 입력 형식을 만들었다.\n",
        "        encoder_text = f\"{target_style_name} 말투로 변환:{text1}\"\n",
        "        decoder_text = f\"{text2}{self.tokenizer.eos_token}\"\n",
        "        model_inputs = self.tokenizer(encoder_text, max_length=64, truncation=True)\n",
        "\n",
        "        with self.tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(decoder_text, max_length=64, truncation=True)\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        del model_inputs['token_type_ids']\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "# 만들어뒀던 데이터셋을 불러와서 데이터프레임을 만들어준다\n",
        "def make_df(data_root):\n",
        "    df = pd.read_csv(f'{data_root}/data_fixed.tsv',sep='\\t')\n",
        "    # 주로 쓰이는 방법 같지는 않지만,, Train Data와 Test Data를 8:2 비율로 나눠준다\n",
        "    rate=int(len(df)*0.1)\n",
        "    df_train,df_test = df[rate:],df[:rate]\n",
        "\n",
        "    print(f'Train DataFrame length : {len(df_train)},Test DataFrame length : {len(df_test)}')\n",
        "    return df_train,df_test\n",
        "\n",
        "def make_dataset(df):\n",
        "    df_train,df_test = df\n",
        "\n",
        "    train_dataset = TextStyleTransferDataset(df_train,tokenizer)\n",
        "    test_dataset = TextStyleTransferDataset(df_test,tokenizer)\n",
        "\n",
        "    return train_dataset,test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIYWSYisS5Wc"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T07:10:12.561654Z",
          "iopub.status.busy": "2024-03-22T07:10:12.561184Z",
          "iopub.status.idle": "2024-03-22T07:10:12.718974Z",
          "shell.execute_reply": "2024-03-22T07:10:12.718040Z",
          "shell.execute_reply.started": "2024-03-22T07:10:12.561624Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSScwHv8S5Wc",
        "outputId": "15deafe0-a6a4-4e62-dd22-9f71e287ed92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataFrame length : 302736,Test DataFrame length : 33637\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainingArguments,Seq2SeqTrainer,\\\n",
        "                         DataCollatorForSeq2Seq\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "df = make_df(data_root)\n",
        "train_dataset,test_dataset=make_dataset(df)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer, model=model\n",
        ")\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    **args,\n",
        "    output_dir=model_path,\n",
        "    )\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T06:38:01.967894Z",
          "iopub.status.busy": "2024-03-22T06:38:01.967557Z",
          "iopub.status.idle": "2024-03-22T06:38:01.974395Z",
          "shell.execute_reply": "2024-03-22T06:38:01.973415Z",
          "shell.execute_reply.started": "2024-03-22T06:38:01.967865Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFFZb_e7S5Wc",
        "outputId": "914f82e1-e8d2-449b-eb6a-a76731f1031f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<transformers.trainer_seq2seq.Seq2SeqTrainer object at 0x78cfdb77ef50>\n"
          ]
        }
      ],
      "source": [
        "print(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T07:10:14.646949Z",
          "iopub.status.busy": "2024-03-22T07:10:14.646072Z",
          "iopub.status.idle": "2024-03-22T07:10:29.290712Z",
          "shell.execute_reply": "2024-03-22T07:10:29.289697Z",
          "shell.execute_reply.started": "2024-03-22T07:10:14.646917Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "hlZVdWSzS5Wc",
        "outputId": "e5143b63-974c-484f-d35b-6c5245c3531d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28383' max='28383' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28383/28383 2:26:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.053000</td>\n",
              "      <td>0.080377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.033700</td>\n",
              "      <td>0.072007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습 진행\n",
        "try:\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    print(f\"Failed to train model caused by {e}\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T06:53:19.893944Z",
          "iopub.status.busy": "2024-03-22T06:53:19.892889Z",
          "iopub.status.idle": "2024-03-22T06:53:20.909879Z",
          "shell.execute_reply": "2024-03-22T06:53:20.908811Z",
          "shell.execute_reply.started": "2024-03-22T06:53:19.893892Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkQkptlhS5Wc",
        "outputId": "244fe34f-e4ce-4a49-b4ba-5019a65981f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    trainer.save_model(model_path)\n",
        "    print(\"Model saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to save model caused by {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LwSoprlS5Wd"
      },
      "source": [
        "## Test (Validation)\n",
        "\n",
        "AI hub에 있는 텍스트 데이터에는 테스트용(라벨 없는거)이 따로 없긴 하다.\n",
        "그래서 이전에 그냥 trainset에서 8:2로 split시켜서 validation용으로 퉁치긴 했지만\n",
        "일단 나중에 오디오 -> 텍스트(STT)실행하고 나서 어차피 데이터는 바꿔서 껴야되기 때문에 그때 validation도 상황에 맞게 데이터 변경해야함\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T06:53:21.732226Z",
          "iopub.status.busy": "2024-03-22T06:53:21.731506Z",
          "iopub.status.idle": "2024-03-22T06:53:22.742029Z",
          "shell.execute_reply": "2024-03-22T06:53:22.740788Z",
          "shell.execute_reply.started": "2024-03-22T06:53:21.732182Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSc4Q-d7S5Wd",
        "outputId": "b3b80ab2-c17a-443d-bccb-27cc220ac282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlg_pipeline=pipeline('text2text-generation',model=model_path,tokenizer=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-22T06:53:23.715213Z",
          "iopub.status.busy": "2024-03-22T06:53:23.714501Z",
          "iopub.status.idle": "2024-03-22T06:53:43.974263Z",
          "shell.execute_reply": "2024-03-22T06:53:43.973264Z",
          "shell.execute_reply.started": "2024-03-22T06:53:23.715164Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHZOO1h3S5Wd",
        "outputId": "ea64bdc8-faf0-4435-989f-9b722c31d0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write 'q' to exit\n",
            "Dialect to translate(입력받을 사투리) : 뭐라카노\n",
            "Translated Standard (표준어로 출력): 뭐라고 하지\n",
            "Dialect to translate(입력받을 사투리) : 지금 시간이 몇신데\n",
            "Translated Standard (표준어로 출력): 지금 시간이 몇신데\n",
            "Dialect to translate(입력받을 사투리) : 정구지랑 찌짐이랑 해서\n",
            "Translated Standard (표준어로 출력): 부추랑 지짐이랑 해서\n",
            "Dialect to translate(입력받을 사투리) : 쫌 쫌생이 같이 굴지 마라\n",
            "Translated Standard (표준어로 출력): 조금 조금생이 같이 굴지 마라\n",
            "Dialect to translate(입력받을 사투리) : 먼 놈의 정구지가 이래 맛있노\n",
            "Translated Standard (표준어로 출력): 먼 놈의 부추가 이래 맛있니\n",
            "Dialect to translate(입력받을 사투리) : 저 할마시가 죽을 때가 됐나 와 저카노\n",
            "Translated Standard (표준어로 출력): 저 할머니가 죽을 때가 됐나 왜 저러나\n",
            "Dialect to translate(입력받을 사투리) : 가가 가가\n",
            "Translated Standard (표준어로 출력): 걔가 걔가\n",
            "Dialect to translate(입력받을 사투리) : 가가가 가\n",
            "Translated Standard (표준어로 출력): 걔가 걔\n",
            "Dialect to translate(입력받을 사투리) : 가가가가\n",
            "Translated Standard (표준어로 출력): 걔가서\n",
            "Dialect to translate(입력받을 사투리) : 끝낫제?\n",
            "Translated Standard (표준어로 출력): 끝났지?\n",
            "Dialect to translate(입력받을 사투리) : 오매\n",
            "Translated Standard (표준어로 출력): 아주\n",
            "Dialect to translate(입력받을 사투리) : 혼자 하믄 재미없다카이\n",
            "Translated Standard (표준어로 출력): 혼자 하면 재미없다고 하니\n",
            "Dialect to translate(입력받을 사투리) : q\n"
          ]
        }
      ],
      "source": [
        "def generate_text(pipe, text, num_return_sequences, max_length):\n",
        "    target_style_name = \"표준어\"\n",
        "    text = f\"{target_style_name} 말투로 변환:{text}\"\n",
        "    out = pipe(text, num_return_sequences=num_return_sequences, max_length=max_length)\n",
        "    #num_return_sequences의 값에 따라서 반환되는 텍스트의 개수가 바뀐다. 만약 3으로 지정했다면 길이가 3인 리스트에 담겨서 값이 반환될 것임!\n",
        "    return [x['generated_text'] for x in out]\n",
        "\n",
        "\n",
        "print(\"Write 'q' to exit\")\n",
        "while True:\n",
        "    src_text=input(\"Dialect to translate(입력받을 사투리) : \")\n",
        "    if src_text == 'q':\n",
        "        break\n",
        "    target_text_ko=generate_text(nlg_pipeline,src_text,num_return_sequences=1,max_length=64)[0]\n",
        "    print(f\"Translated Standard (표준어로 출력): {target_text_ko}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"/content/drive/MyDrive/daiv_audio/kobart_epoch3_trained.pth\")"
      ],
      "metadata": {
        "id": "8y-LbMQ6sL3J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJgPCG-R4a2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4627269,
          "sourceId": 7883354,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4628075,
          "sourceId": 7884409,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}